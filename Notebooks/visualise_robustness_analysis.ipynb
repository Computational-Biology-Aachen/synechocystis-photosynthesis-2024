{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modelbase.ode import Simulator\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../Code\")\n",
    "\n",
    "# Import model functions\n",
    "from get_current_model import get_model\n",
    "from function_residuals import calculate_residuals, calculate_residuals_ePathways, integrator_kwargs\n",
    "from robustness_fit_parameters import get_fitting_parameter_dict, p_names\n",
    "import functions_light_absorption as lip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcpar = pd.read_csv(\"../Results/montecarlo_202405112232_params.csv\", index_col=0)\n",
    "mcres = pd.read_csv(\"../Results/montecarlo_202405131529_results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "mcres_outcomes = pd.DataFrame(index=mcres.index, columns=[\"success\", \"failed\", \"time-out\"])\n",
    "\n",
    "mcres_outcomes[\"timeout\"] = np.isnan(mcres).any(axis=1)\n",
    "mcres_outcomes[\"failed\"] = np.isinf(mcres).any(axis=1)\n",
    "mcres_outcomes[\"success\"] = mcres_succ = np.invert(np.logical_or(mcres_outcomes[\"timeout\"], mcres_outcomes[\"failed\"]))\n",
    "\n",
    "print(f\"Full runs: {mcres_outcomes['success'].sum()}\")\n",
    "print(f\"Time-outs: {mcres_outcomes['timeout'].sum()}\")\n",
    "print(f\"Failed: {mcres_outcomes['failed'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find simulations with improved objective functions\n",
    "mcres_improved = mcres.copy()\n",
    "mcres_improved = mcres_improved - mcres_improved.iloc[0,:]\n",
    "\n",
    "# Find simulations with improvement in all objective functions\n",
    "mcres_outcomes[\"total_better\"] = (mcres_improved < 0).all(axis=1)\n",
    "\n",
    "# Find simulations with improvement in all objective functions\n",
    "mcres_outcomes[\"similar_pareto\"] = (mcres_improved < 0.1).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcres_outcomes[\"similar_pareto\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcpar_flat = mcpar.copy()\n",
    "\n",
    "# Evaluate the strings in fluo_influence as their dict values are stored as string\n",
    "# Not a goor practice\n",
    "mcpar_flat[\"fluo_influence\"] = mcpar_flat[\"fluo_influence\"].apply(eval)\n",
    "\n",
    "# Unpack the fluo influence parameters \n",
    "_fluo_influence = pd.DataFrame(mcpar_flat[\"fluo_influence\"].to_dict()).T\n",
    "_fluo_influence.columns = \"fluo_influence_\" + _fluo_influence.columns\n",
    "\n",
    "# Append them to the parameters\n",
    "mcpar_flat = pd.concat([mcpar_flat, _fluo_influence], axis=1)\n",
    "mcpar_flat = mcpar_flat.drop(\"fluo_influence\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the parameters\n",
    "mcpar_fnorm = mcpar_flat.copy()\n",
    "mcpar_fnorm = mcpar_fnorm / mcpar_fnorm.iloc[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the overall distribution\n",
    "\n",
    "fig, axes = plt.subplots(1, mcres.shape[1], figsize=(15,5), sharey=True)\n",
    "\n",
    "for (nam, val), ax in zip(mcres.T.iterrows(), axes.flatten()):\n",
    "    ax.hist(val[mcres_succ], bins=50)\n",
    "    ax.set_xlabel(nam)\n",
    "    ax.axvline(mcres.loc[0,nam], c=\"k\")\n",
    "\n",
    "fig.suptitle(\"Distribution of residual functions\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise comparison of model objectives\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    mcres.shape[1],\n",
    "    mcres.shape[1],\n",
    "    figsize=(10,10),\n",
    "    sharey=False,\n",
    "    sharex=\"col\"\n",
    ")\n",
    "\n",
    "# Plot the pariwise comparisons as scatter plots\n",
    "for i, (nam_i, val_i) in enumerate(mcres.T.iterrows()):\n",
    "    for j, (nam_j, val_j) in enumerate(mcres.T.iterrows()):\n",
    "        # On the diagonal plot histograms\n",
    "        if i==j:\n",
    "            axes[i,j].hist(val_i[mcres_succ], bins=50)\n",
    "        \n",
    "        elif i>j:\n",
    "            axes[i,j].plot(\n",
    "                val_j,\n",
    "                val_i,\n",
    "                marker=\"o\",\n",
    "                markersize=1,\n",
    "                linestyle=\"\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            axes[i,j].remove()\n",
    "\n",
    "        if j!=0:\n",
    "            axes[i,j].get_yaxis().set_ticks([])\n",
    "\n",
    "    ax.hist(val[mcres_succ], bins=50)\n",
    "    ax.set_xlabel(nam)\n",
    "    ax.axvline(mcres.loc[0,nam], c=\"k\")\n",
    "\n",
    "fig.suptitle(\"Distribution of objective functions\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_residuals_ePathways(\n",
    "#     parameter_update={},\n",
    "#     Schuurmans=None,\n",
    "#     Benschop_CO2pps=None,\n",
    "#     Benschop_CO2uMs=None,\n",
    "#     Benschop2003_low=None,\n",
    "#     experiment_select_435=None,\n",
    "#     absorption_coef_PAM435=None,\n",
    "#     data_points_435=None,\n",
    "#     strain_params=None,\n",
    "#     data_points_val=None,\n",
    "#     point_timing=None,\n",
    "#     fraction_simulated_points=1,\n",
    "#     integrator_kwargs=integrator_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(mcres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise with single output\n",
    "nrows = 7\n",
    "fig, _axes = plt.subplots(nrows, int(np.ceil((mcpar_flat.shape[1]+1) / nrows)), figsize=(15,15))\n",
    "axes = _axes.flatten()\n",
    "\n",
    "plot_type = \"scatter\"\n",
    "\n",
    "# Plot the overall distribution\n",
    "axes[0].hist(mcres[mcres_succ], bins=100, orientation=\"horizontal\")\n",
    "axes[0].set_xlabel(\"Frequency\")\n",
    "\n",
    "for i, (nam, vals) in enumerate(mcpar_flat.T.iterrows()):\n",
    "\n",
    "    if plot_type == \"scatter\":\n",
    "        axes[i+1].plot(\n",
    "            vals[mcres_succ],\n",
    "            mcres[mcres_succ],\n",
    "            marker=\"o\",\n",
    "            ls=\"\",\n",
    "            markersize=1\n",
    "        )\n",
    "        axes[i+1].set_xscale(\"log\")\n",
    "    elif plot_type == \"hex\":\n",
    "        axes[i+1].hexbin(\n",
    "            np.log10(vals[mcres_succ]),\n",
    "            mcres[mcres_succ],\n",
    "        )\n",
    "\n",
    "    # axes[i+1].plot(\n",
    "    #     vals[0],\n",
    "    #     mcres[0],\n",
    "    #     marker=\"o\",\n",
    "    #     ls=\"\",\n",
    "    #     markersize=3,\n",
    "    #     c=\"red\"\n",
    "    # )\n",
    "\n",
    "    axes[i+1].set_xlabel(nam)\n",
    "\n",
    "# Add the y label\n",
    "_axes[nrows//2, 0].set_ylabel(\"Root mean squared residuals [AU]\")\n",
    "\n",
    "# Add a title and improve the layout\n",
    "fig.suptitle(\"Residuals under parameter variation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.show(fig)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mcpar_fnorm[mcres_succ].to_numpy()\n",
    "y = mcres[mcres_succ]\n",
    "\n",
    "model = LinearRegression(fit_intercept=True).fit(x,y)\n",
    "\n",
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hexbin(\n",
    "        np.log10(vals[mcres_succ]),\n",
    "        mcres[mcres_succ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mcres[mcres_succ].plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcres.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(mcpar[mcres_succ].iloc[:,0]).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_mutation = 916\n",
    "par = mcpar.loc[selected_mutation,:].to_dict()\n",
    "par[\"fluo_influence\"] = eval(par[\"fluo_influence\"])\n",
    "\n",
    "m,y0 = get_model(verbose=False, check_consistency=False)\n",
    "m.update_parameters(par)\n",
    "m.update_parameter(\"pfd\", lip.light_gaussianLED(670, 246))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Simulator(m)\n",
    "s.initialise(y0)\n",
    "s.simulate(1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Results/minimise_202405081435_results.pickle\", \"rb\") as f:\n",
    "    fit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_optim = get_fitting_parameter_dict(fit.x, p_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = calculate_residuals(p_optim, save_intermediates=False, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pd.DataFrame(index=np.arange(7), columns=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series({1:2, 2:4, 3:6})\n",
    "test2 = pd.Series({2:\"a\", 3:\"b\", 1:\"c\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../Results/202404171132_CO2production_wi.dill\").is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join(test.values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.loc[0,:] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.loc[1,:] = test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synphot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
